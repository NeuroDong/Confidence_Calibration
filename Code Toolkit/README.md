# About
A comprehensive code toolkit for confidence calibration. It provides codes for:
 - Data acquisition, see [CaliData](https://github.com/NeuroDong/Confidence_Calibration/tree/main/Code%20Toolkit/CaliData).
 - Calibration methods, see [CaliMethod](https://github.com/NeuroDong/Confidence_Calibration/tree/main/Code%20Toolkit/CaliMethod).
 - Calibration metrics, see [CaliMetric](https://github.com/NeuroDong/Confidence_Calibration/tree/main/Code%20Toolkit/CaliMetric).
 - Calibration visualization, see [CaliShow](https://github.com/NeuroDong/Confidence_Calibration/tree/main/Code%20Toolkit/CaliShow).

# Included Code

 - [CaliData](https://github.com/NeuroDong/Confidence_Calibration/tree/main/Code%20Toolkit/CaliData)
   - [Real_Data](https://github.com/NeuroDong/Confidence_Calibration/tree/main/Code%20Toolkit/CaliData/Real_Data)
     - [Logit_Datasets_for_Neural_Networks.py](https://github.com/NeuroDong/Confidence_Calibration/tree/main/Code%20Toolkit/CaliData/Real_Data/Logit_Datasets_for_Neural_Networks.py)
   - [Simulated_Data](https://github.com/NeuroDong/Confidence_Calibration/tree/main/Code%20Toolkit/CaliData/Simulated_Data)
     - [Binomial_Process_Sampling.py](https://github.com/NeuroDong/Confidence_Calibration/tree/main/Code%20Toolkit/CaliData/Simulated_Data/Binomial_Process_Sampling.py)
 - [CaliMethod](https://github.com/NeuroDong/Confidence_Calibration/tree/main/Code%20Toolkit/CaliMethod)
   - [General_loss]()
   - [In_training]()
   - [Post_hoc]()
 - [CaliMetric](https://github.com/NeuroDong/Confidence_Calibration/tree/main/Code%20Toolkit/CaliMetric)
   - [Multi_Calibration]()
   - [Top_Label_Calibration]()
 - [CaliShow](https://github.com/NeuroDong/Confidence_Calibration/tree/main/Code%20Toolkit/CaliShow)
   - [validity_plot.py]()